{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wan-ATI: High-Quality Video Generation from a Single Image\n",
    "\n",
    "This notebook is a converted version of the original inference script for the Wan-ATI model. It allows you to generate a video from a text prompt and a single starting image.\n",
    "\n",
    "### References:\n",
    "- **Original Script:** Provided by the user.\n",
    "- **Project Page / GitHub (Hypothetical):** [Link to Project]\n",
    "\n",
    "### How to Use:\n",
    "1.  **Setup Environment:** Make sure you have all the required libraries installed (`wan`, `torch`, `Pillow`, etc.) and have downloaded the model checkpoints.\n",
    "2.  **Configure Parameters:** In the \"1. Configuration\" section below, you **must** set the `ckpt_dir` to the path where you saved the checkpoints. You also need to provide your own `prompt`, `image`, and optionally `track` file.\n",
    "3.  **Run All Cells:** Click `Kernel -> Restart & Run All` to execute the notebook and generate your video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024-2025 The Alibaba Wan Team Authors. All rights reserved.\n",
    "# Copyright (c) 2024-2025 Bytedance Ltd. and/or its affiliates\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "from wan.utils.motion import get_tracks_inference\n",
    "from wan.utils.utils import cache_video, cache_image\n",
    "from wan.utils.prompt_extend import DashScopePromptExpander, QwenPromptExpander\n",
    "from wan.configs import MAX_AREA_CONFIGS, SIZE_CONFIGS, SUPPORTED_SIZES, WAN_CONFIGS\n",
    "import wan\n",
    "from PIL import Image\n",
    "import torch.distributed as dist\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from types import SimpleNamespace\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "All parameters that were previously command-line arguments are defined here as regular variables. **Modify the values in the next cell** to control the generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SimpleNamespace()\n",
    "\n",
    "# ======================================================================================\n",
    "# !! REQUIRED: USER INPUTS !!\n",
    "# ======================================================================================\n",
    "\n",
    "# -- The path to the checkpoint directory. --\n",
    "# !! YOU MUST CHANGE THIS to the location of your downloaded models.\n",
    "args.ckpt_dir = \"./Wan2.1-ATI-14B-480P\"\n",
    "\n",
    "# -- The prompt to generate the video from. --\n",
    "# Can also be a path to a .yaml file for batch processing.\n",
    "args.prompt = \"A tranquil koi pond edged by mossy stone, with lily pads drifting on the surface and several orange\\u2011and\\u2011white koi fish gliding beneath.\"\n",
    "\n",
    "# -- [image to video] The input image to generate the video from. --\n",
    "args.image = \"./examples/images/fish.jpg\" # e.g., 'assets/images/cat.png'\n",
    "\n",
    "# -- The stored point trajectory to generate the video. (Optional) --\n",
    "args.track = \"./examples/tracks/fish.pth\" # e.g., 'assets/tracks/cat.txt'\n",
    "\n",
    "# ======================================================================================\n",
    "# Core Generation Parameters\n",
    "# ======================================================================================\n",
    "\n",
    "# The task to run.\n",
    "args.task = \"ati-14B\" # choices: list(WAN_CONFIGS.keys())\n",
    "\n",
    "# The area (width*height) of the generated video.\n",
    "args.size = \"832*480\" # choices: list(SIZE_CONFIGS.keys())\n",
    "\n",
    "# How many frames to sample. Should be 4n+1. If None, defaults to 81 for video tasks.\n",
    "args.frame_num = None\n",
    "\n",
    "# The file to save the generated video to. If None, a name is generated automatically.\n",
    "args.save_file = None # e.g., \"my_video.mp4\"\n",
    "\n",
    "# The seed to use. -1 for a random seed.\n",
    "args.base_seed = 42\n",
    "\n",
    "# ======================================================================================\n",
    "# Advanced Sampling Parameters\n",
    "# ======================================================================================\n",
    "\n",
    "# The solver used to sample.\n",
    "args.sample_solver = 'unipc' # choices: ['unipc', 'dpm++']\n",
    "\n",
    "# The sampling steps. If None, defaults to 10.\n",
    "args.sample_steps = 10\n",
    "\n",
    "# Sampling shift factor for flow matching schedulers. If None, defaults to 5.0.\n",
    "args.sample_shift = None \n",
    "\n",
    "# Classifier free guidance scale.\n",
    "args.sample_guide_scale = 5.0\n",
    "\n",
    "# ======================================================================================\n",
    "# System & Performance Parameters (Advanced)\n",
    "# ======================================================================================\n",
    "\n",
    "# Whether to offload the model to CPU after each forward pass. Reduces GPU memory.\n",
    "# If None, it will be set to True for single-GPU and False for multi-GPU.\n",
    "args.offload_model = None\n",
    "\n",
    "# The size of the ulysses parallelism in DiT (for multi-GPU).\n",
    "args.ulysses_size = 1\n",
    "\n",
    "# The size of the ring attention parallelism in DiT (for multi-GPU).\n",
    "args.ring_size = 1\n",
    "\n",
    "# Whether to use FSDP for T5 (for multi-GPU).\n",
    "args.t5_fsdp = False\n",
    "\n",
    "# Whether to place T5 model on CPU.\n",
    "args.t5_cpu = False\n",
    "\n",
    "# Whether to use FSDP for DiT (for multi-GPU).\n",
    "args.dit_fsdp = False\n",
    "\n",
    "# ======================================================================================\n",
    "# Prompt Extension Parameters (Optional)\n",
    "# ======================================================================================\n",
    "args.use_prompt_extend = False\n",
    "args.prompt_extend_method = \"local_qwen\" # choices: [\"dashscope\", \"local_qwen\"]\n",
    "args.prompt_extend_model = None\n",
    "args.prompt_extend_target_lang = \"zh\" # choices: [\"zh\", \"en\"]\n",
    "\n",
    "# ======================================================================================\n",
    "# Other Task-Specific Parameters (leave as None if not used)\n",
    "# ======================================================================================\n",
    "args.src_video = None\n",
    "args.src_mask = None\n",
    "args.src_ref_images = None\n",
    "args.first_frame = None\n",
    "args.last_frame = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate_args(args):\n",
    "    \"\"\"Validates arguments and sets defaults if they are None.\"\"\"\n",
    "    # Basic check\n",
    "    assert args.ckpt_dir is not None, \"Please specify the checkpoint directory in the cell above.\"\n",
    "    assert os.path.exists(args.ckpt_dir), f\"Checkpoint directory not found at: {args.ckpt_dir}\"\n",
    "    assert args.task in WAN_CONFIGS, f\"Unsupport task: {args.task}\"\n",
    "\n",
    "    # The default sampling steps are 40 for image-to-video tasks and 50 for text-to-video tasks.\n",
    "    if args.sample_steps is None:\n",
    "        args.sample_steps = 10\n",
    "\n",
    "    if args.sample_shift is None:\n",
    "        args.sample_shift = 5.0\n",
    "\n",
    "    # The default number of frames are 1 for text-to-image tasks and 81 for other tasks.\n",
    "    if args.frame_num is None:\n",
    "        args.frame_num = 1 if \"t2i\" in args.task else 81\n",
    "\n",
    "    # T2I frame_num check\n",
    "    if \"t2i\" in args.task:\n",
    "        assert args.frame_num == 1, f\"Unsupport frame_num {args.frame_num} for task {args.task}\"\n",
    "\n",
    "    if args.base_seed is None or args.base_seed < 0:\n",
    "        args.base_seed = random.randint(0, sys.maxsize)\n",
    "        \n",
    "    # Size check\n",
    "    assert args.size in SUPPORTED_SIZES[\n",
    "        args.task], f\"Unsupport size {args.size} for task {args.task}, supported sizes are: {', '.join(SUPPORTED_SIZES[args.task])}\"\n",
    "    \n",
    "    # Input file checks\n",
    "    if args.prompt and not args.prompt.endswith('.yaml'):\n",
    "        assert args.image is not None, \"Please provide an input image via 'args.image'\"\n",
    "        assert os.path.exists(args.image), f\"Input image not found at: {args.image}\"\n",
    "        if args.track:\n",
    "             assert os.path.exists(args.track), f\"Track file not found at: {args.track}\"\n",
    "    elif args.prompt and args.prompt.endswith('.yaml'):\n",
    "        assert os.path.exists(args.prompt), f\"YAML prompt file not found at: {args.prompt}\"\n",
    "    else:\n",
    "        raise ValueError(\"Please provide a text prompt or a YAML file for batch processing.\")\n",
    "\n",
    "def _init_logging(rank):\n",
    "    \"\"\"Initializes logging for the notebook.\"\"\"\n",
    "    # Remove all handlers associated with the root logger\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "        \n",
    "    if rank == 0:\n",
    "        # set format\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format=\"[%(asctime)s] %(levelname)s: %(message)s\",\n",
    "            handlers=[logging.StreamHandler(stream=sys.stdout)])\n",
    "    else:\n",
    "        logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Execution\n",
    "\n",
    "The following cell contains the main logic to set up the environment, load the model, and run the generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-02 20:00:16,725] INFO: Generation job args: {'ckpt_dir': './Wan2.1-ATI-14B-480P', 'prompt': 'A tranquil koi pond edged by mossy stone, with lily pads drifting on the surface and several orange‑and‑white koi fish gliding beneath.', 'image': './examples/images/fish.jpg', 'track': './examples/tracks/fish.pth', 'task': 'ati-14B', 'size': '832*480', 'frame_num': 81, 'save_file': None, 'base_seed': 42, 'sample_solver': 'unipc', 'sample_steps': 10, 'sample_shift': 5.0, 'sample_guide_scale': 5.0, 'offload_model': True, 'ulysses_size': 1, 'ring_size': 1, 't5_fsdp': False, 't5_cpu': False, 'dit_fsdp': False, 'use_prompt_extend': False, 'prompt_extend_method': 'local_qwen', 'prompt_extend_model': None, 'prompt_extend_target_lang': 'zh', 'src_video': None, 'src_mask': None, 'src_ref_images': None, 'first_frame': None, 'last_frame': None}\n",
      "[2025-07-02 20:00:16,726] INFO: Generation model config: {'__name__': 'Config: Wan I2V 14B', 't5_model': 'umt5_xxl', 't5_dtype': torch.bfloat16, 'text_len': 512, 'param_dtype': torch.bfloat16, 'num_train_timesteps': 1000, 'sample_fps': 16, 'sample_neg_prompt': '镜头晃动，色调艳丽，过曝，静态，细节模糊不清，字幕，风格，作品，画作，画面，静止，整体发灰，最差质量，低质量，JPEG压缩残留，丑陋的，残缺的，多余的手指，画得不好的手部，画得不好的脸部，畸形的，毁容的，形态畸形的肢体，手指融合，静止不动的画面，杂乱的背景，三条腿，背景人很多，倒着走', 't5_checkpoint': 'models_t5_umt5-xxl-enc-bf16.pth', 't5_tokenizer': 'google/umt5-xxl', 'clip_model': 'clip_xlm_roberta_vit_h_14', 'clip_dtype': torch.float16, 'clip_checkpoint': 'models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth', 'clip_tokenizer': 'xlm-roberta-large', 'vae_checkpoint': 'Wan2.1_VAE.pth', 'vae_stride': (4, 8, 8), 'patch_size': (1, 2, 2), 'dim': 5120, 'ffn_dim': 13824, 'freq_dim': 256, 'num_heads': 40, 'num_layers': 40, 'window_size': (-1, -1), 'qk_norm': True, 'cross_attn_norm': True, 'eps': 1e-06}\n",
      "[2025-07-02 20:00:16,730] INFO: Creating WanATI pipeline...\n",
      "[2025-07-02 20:01:28,912] INFO: loading ./Wan2.1-ATI-14B-480P/models_t5_umt5-xxl-enc-bf16.pth\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Validate Arguments & Environment Setup ---\n",
    "_validate_args(args)\n",
    "\n",
    "# For a standard notebook environment, we simulate a single-process run.\n",
    "rank = 0\n",
    "world_size = 1\n",
    "local_rank = 0\n",
    "device = 0\n",
    "os.environ['RANK'] = str(rank)\n",
    "os.environ['WORLD_SIZE'] = str(world_size)\n",
    "os.environ['LOCAL_RANK'] = str(local_rank)\n",
    "\n",
    "_init_logging(rank)\n",
    "\n",
    "if args.offload_model is None:\n",
    "    args.offload_model = False if world_size > 1 else True\n",
    "    logging.info(f\"offload_model is not specified, set to {args.offload_model}.\")\n",
    "\n",
    "# This notebook assumes a single-GPU/CPU setup. Distributed features are disabled.\n",
    "assert not (args.t5_fsdp or args.dit_fsdp), \"FSDP is not supported in this notebook setup.\"\n",
    "assert not (args.ulysses_size > 1 or args.ring_size > 1), \"Context parallelism is not supported in this notebook setup.\"\n",
    "\n",
    "# --- 2. Setup Prompt Expander (if used) ---\n",
    "if args.use_prompt_extend:\n",
    "    if args.prompt_extend_method == \"dashscope\":\n",
    "        prompt_expander = DashScopePromptExpander(model_name=args.prompt_extend_model, is_vl=True)\n",
    "    elif args.prompt_extend_method == \"local_qwen\":\n",
    "        prompt_expander = QwenPromptExpander(model_name=args.prompt_extend_model, is_vl=True, device=device)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Unsupport prompt_extend_method: {args.prompt_extend_method}\")\n",
    "\n",
    "# --- 3. Load Model and Prepare Inputs ---\n",
    "cfg = WAN_CONFIGS[args.task]\n",
    "logging.info(f\"Generation job args: {vars(args)}\")\n",
    "logging.info(f\"Generation model config: {cfg}\")\n",
    "\n",
    "torch.manual_seed(args.base_seed)\n",
    "random.seed(args.base_seed)\n",
    "\n",
    "if args.prompt.endswith('.yaml'):\n",
    "    inputs_ = []\n",
    "    fl_list = yaml.safe_load(open(args.prompt))\n",
    "    for line in fl_list:\n",
    "        inputs_.append((line['image'], line['text'].strip(), line['track']))\n",
    "else:\n",
    "    inputs_ = [(args.image, args.prompt, args.track)]\n",
    "\n",
    "logging.info(\"Creating WanATI pipeline...\")\n",
    "wan_ati = wan.WanATI(\n",
    "    config=cfg,\n",
    "    checkpoint_dir=args.ckpt_dir,\n",
    "    device_id=device,\n",
    "    rank=rank,\n",
    "    t5_fsdp=args.t5_fsdp,\n",
    "    dit_fsdp=args.dit_fsdp,\n",
    "    use_usp=(args.ulysses_size > 1 or args.ring_size > 1),\n",
    "    t5_cpu=args.t5_cpu,\n",
    ")\n",
    "\n",
    "# --- 4. Run Generation Loop ---\n",
    "for ii, input_ in enumerate(inputs_):\n",
    "    # Determine save file name\n",
    "    current_save_file = args.save_file\n",
    "    if current_save_file is None:\n",
    "        formatted_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        if args.prompt.endswith(\".yaml\"):\n",
    "            formatted_prompt = f\"{ii:02d}\"\n",
    "        else:\n",
    "            # Sanitize prompt for use in filename\n",
    "            sanitized_prompt = \"\".join([c for c in args.prompt if c.isalpha() or c.isdigit() or c==' ']).rstrip()\n",
    "            formatted_prompt = sanitized_prompt.replace(\" \", \"_\").replace(\"/\", \"_\")[:50]\n",
    "        suffix = '.mp4'\n",
    "        current_save_file = f\"output/{args.task}_{args.size.replace('*','x')}_{formatted_prompt}_{formatted_time}{suffix}\"\n",
    "    elif '%' in current_save_file:\n",
    "        current_save_file = current_save_file % ii\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(current_save_file), exist_ok=True)\n",
    "        \n",
    "    if os.path.exists(current_save_file):\n",
    "        logging.info(f\"File {current_save_file} already exists, skipping.\")\n",
    "        continue\n",
    "\n",
    "    image, prompt, tracks = input_\n",
    "    logging.info(f\"Input prompt: {prompt}\")\n",
    "    logging.info(f\"Input image: {image}\")\n",
    "\n",
    "    img = Image.open(image).convert(\"RGB\")\n",
    "\n",
    "    width, height = img.size\n",
    "    tracks = get_tracks_inference(tracks, height, width)\n",
    "\n",
    "    if args.use_prompt_extend:\n",
    "        logging.info(\"Extending prompt ...\")\n",
    "        prompt_output = prompt_expander(\n",
    "            prompt,\n",
    "            tar_lang=args.prompt_extend_target_lang,\n",
    "            image=img,\n",
    "            seed=args.base_seed)\n",
    "        if prompt_output.status == False:\n",
    "            logging.info(f\"Extending prompt failed: {prompt_output.message}\")\n",
    "            logging.info(\"Falling back to original prompt.\")\n",
    "        else:\n",
    "            prompt = prompt_output.prompt\n",
    "        logging.info(f\"Extended prompt: {prompt}\")\n",
    "\n",
    "    logging.info(\"Generating video ...\")\n",
    "    video = wan_ati.generate(\n",
    "        prompt,\n",
    "        img,\n",
    "        tracks,\n",
    "        max_area=MAX_AREA_CONFIGS[args.size],\n",
    "        frame_num=args.frame_num,\n",
    "        shift=args.sample_shift,\n",
    "        sample_solver=args.sample_solver,\n",
    "        sampling_steps=args.sample_steps,\n",
    "        guide_scale=args.sample_guide_scale,\n",
    "        seed=args.base_seed,\n",
    "        offload_model=args.offload_model)\n",
    "\n",
    "    logging.info(f\"Saving generated video to {current_save_file}\")\n",
    "    cache_video(\n",
    "        tensor=video[None],\n",
    "        save_file=current_save_file,\n",
    "        fps=cfg.sample_fps,\n",
    "        nrow=1,\n",
    "        normalize=True,\n",
    "        value_range=(-1, 1))\n",
    "    \n",
    "    # Display video in notebook (optional)\n",
    "    try:\n",
    "        from IPython.display import Video, display\n",
    "        display(Video(current_save_file, embed=True, width=400))\n",
    "    except ImportError:\n",
    "        print(\"Install IPython to display the video directly in the notebook.\")\n",
    "    \n",
    "logging.info(\"Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
